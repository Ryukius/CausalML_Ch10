---
title: "Causal ML Book Ch10.2-10.3"
author: "Ryuki Kobayashi"
date: "2024/11/25"
output: html_notebook
---

## 10.2 DML Inference in the Partially Linear Regression Model
### Revisiting the Price Elasticity for Toy Cars

- おもちゃの車の価格弾力性を推定する例
- 価格弾力性の推定について以下の部分線形モデル (Partially Linear Model, PLM) を考える

\begin{equation}
  Y = \alpha D + g(W) + \varepsilon
\end{equation}  

- 各変数は以下の通り
  - $Y$: 販売ランクの逆数の対数 (log-reciprocal-sales-rank)
  - $D$: 価格の対数
  - $W$: 製品属性のベクトル  

#### **Double Machine Learning (DML) を用いた $\alpha$ の推定**
- 高次元な製品属性を使用する
   - $W$: 各製品属性とブランド・サブカテゴリーとの交差項などを含む2083次元の属性ベクトルをとする
- 使用する機械学習アルゴリズム
   - Decision Tree (決定木)
   - Gradient Boosted Trees (勾配ブースティング決定木, 1000 trees)
   - Random Forest (ランダムフォレスト, 2000 trees)
   - Neural Network (ニューラルネットワーク: 200 layers, 20 neurons)


#### Table 10.3: 推定結果

| Model                 | $R^2_D$ | $R^2_Y$ | Estimate  | Std. Err. | 95% CI            |
|-----------------------|-------------|-------------|-----------|-----------|-------------------|
| Tree                  | 0.40        | 0.19        | -0.109    | 0.018     | [-0.143, -0.074]  |
| Boost                 | 0.41        | 0.17        | -0.102    | 0.019     | [-0.139, -0.064]  |
| Forest                | 0.49        | 0.26        | -0.134    | 0.019     | [-0.171, -0.096]  |
| NNet                  | 0.47        | 0.21        | -0.132    | 0.020     | [-0.171, -0.093]  |

**参考: Table4.3**

| Model                 | $R^2_D$ | $R^2_Y$ | Estimate  | Std. Err. | 95% CI            |
|-----------------------|-------------|-------------|-----------|-----------|-------------------|
| OLS ($p = 242$)    | -           | -           | 0.005     | 0.016     | [-0.026, 0.036]   |
| OLS ($p = 2068$)   | -           | -           | -0.003    | 0.021     | [-0.045, 0.039]   |
| OLS ($p = 2072$)   | -           | -           | -0.033    | 0.022     | [-0.076, 0.010]   |
| Double Lasso          | 0.09        | 0.32        | -0.064    | 0.018     | [-0.099, -0.029]  |
| Double Selection      | -           | -           | -0.074    | 0.019     | [-0.111, -0.037]  |
| Desparsified Lasso    | -           | -           | -0.062    | 0.017     | [-0.096, -0.028]  |


- 機械学習は全モデルで価格弾力性の推定値が負かつ信頼区間がゼロを含まず、理論的予測と一致
- 信頼区間の比較: Double Lasso の結果よりさらに負の推定値に
- 決定係数 $R^2$ の比較: Lassoにと比べ、非線形の機械学習モデルがよりも高い $R^2$ を達成
  - 特に、decisin tree や Random Forest が最も高い $R^2$ を達成

$\rightarrow$ 非線形モデルが共変量をより適切にコントロールできることが示唆

---

#### **高次変換の使用: 価格弾力性の非線形性を推定**

- 価格弾力性の非線形性を推定するためエルミート多項式 (Hermite polynomial) を導入
  - 直交する多項式を使用することで、推定を安定させることができる（多重共線性の問題が起こりづらくなる）らしい
    \begin{equation}
    Y = \sum_{j=1}^r \alpha_j T_j(D) + g(W) + \varepsilon
    \end{equation}


```{r, echo=FALSE, out.width="70%",  fig.align = 'center'}
knitr::include_graphics("figure/fig10.5.png")
```


- Figure 10.5: $r = 1, 2, 3, 4$ の場合を比較
  - 弾力性は非線形 $\rightarrow$ 低価格帯で非弾力的、高価格帯ではより弾力的になる
  - あるいは、低価格帯における confounding factor をうまくコントロールできていない



## 10.3 DML Inference in the Interactive Regression Model
### DML Inference on APEs and ATEs

次の interactive regression model (IRM) と propensity score の2つの回帰モデルの組を考える

\begin{align}
  Y & = g_0(D, X) + \varepsilon, \quad \mathbb{E}[\varepsilon | X, D] = 0, \tag{10.3.1}  \\ 
  D & = m_0(X) + \tilde{D}, \quad \mathbb{E}[\tilde{D} | X] = 0. \tag{10.3.2}
\end{align}

- 観測されている変数の組 $W=(Y,D,X)$
  - $Y$ は興味のあるアウトカム
  - $D\in\{0,1\}$ は二値の処置変数 (binary treatment variable)
  - $X$ は共変量 (controls/confounding factors)
- $(10.2.1)$式の部分線形モデル (PLM) の一般化になっている
  - $(10.3.1)$式より、$D$ と $X$ は加法分離的 (additive separable) でなく、共変量 \(X\) によって処置効果 (treatment effect) が非線形に異なる場合を表している
  - $(10.3.2)$式より、$D$ と $X$ は独立ではなく、$D$ は $X$ によって決まることを明示的に表す (confounded)
- 未知の関数 $g_0$ と $m_0$ を推定するために、機械学習アルゴリズムを使用する

**興味のあるパラメタ (parameter of interest): 平均予測効果 (APE)**

\begin{equation}
   \theta_0 = \mathbb{E}[g_0(1, X) - g_0(0, X)] \tag{10.3.3}
\end{equation}
     
- $D \perp Y(d) \mid X$ (無視可能性 ignorability / 条件付き独立性 ignorability) が成立している場合、APE は平均処置効果 (ATE) と一致する


#### **Neyman 直交性を満たす効率的な推定量**

- $g_0$ と $m_0$ の推定に関するわずかな推定誤差が、興味のあるパラメタ $\theta_0$についてのモーメント条件に影響を与えないような推定量 $\varphi_0(W)$ を考えると、

\begin{equation}
   \theta_0 = \mathbb{E}[\varphi_0(W)], \tag{10.3.4}
\end{equation}

- 推定式 $\varphi_0(W)$は、非線形回帰と傾向スコア重み付けを組み合わせたものである

\begin{equation}
  \varphi_0(W) = \varphi_0(Y,D,X) = g_0(1, X) - g_0(0, X) + (Y - g_0(D, X))H_0,
\end{equation}

- 重み$H_0$ は、傾向スコアの逆数によって重み付けされた Horvitz-Thompson weight である
\begin{equation}  
  H_0 = \frac{1(D = 1)}{m_0(X)} - \frac{1(D = 0)}{1 - m_0(X)}.
\end{equation}

- 補足: "doubly robust" な定式化でもある
  - $g_0$ または $m_0$ のいずれかが正確に定式化できていれば、$\theta$ を正確に推定できる


#### **Remark 10.3.1: Regression Adjustment or Propensity ScoreReweighting? Use both**

- 回帰による推定 (Regression adjustment)

\begin{equation}
  \theta_0=\mathrm{E}\left[g_0(1, X)-g_0(0, X)\right],
\end{equation}

- 傾向スコア重み付け (Propensity Score Reweighting)

\begin{equation}
  \theta_0=\mathrm{E}\left[Y H_0\right] .
\end{equation}

- 上記2つの手法は、 Neyman Orthogonality を満たさない


#### **推定アルゴリズム: DML for APEs/ATEs**

1. Cross-fitting
    - データを、ほぼ同じサイズになるよう、ランダムに $\{I_k\}_{k=1}^K$ の fold へと $K$ 分割する
    - フォルド \(k\) を除いたデータで \(g_0\) と \(m_0\) の推定値 $m_{[k]}$ と $g_{[k]}$ を得る
    - 各 fold \(i \in I_k\) について、下記を計算する

     \begin{equation}
     \hat{\varphi}(W_i) = \hat{g}_{[k]}(1, X_i) - \hat{g}_{[k]}(0, X_i) + (Y_i - \hat{g}_{[k]}(D_i, X_i))\hat{H}_i,
     \end{equation}
     ここで:
     \begin{equation}
     \hat{H}_i = \frac{1(D_i = 1)}{\hat{m}_{[k]}(X_i)} - \frac{1(D_i = 0)}{1 - \hat{m}_{[k]}(X_i)}. 
     \end{equation}

3. 推定量から、$\theta$ の推定値を得る
   \begin{equation}
    \hat{\theta} = \frac{1}{n} \sum_{i=1}^n \hat{\varphi}(W_i).
   \end{equation}

4. 標準誤差の計算
   - 分散の推定量
     \begin{equation}
        \hat{V} = \frac{1}{n} \sum_{i=1}^n (\hat{\varphi}(W_i) - \hat{\theta})^2.
     \end{equation}
   - 標準誤差
     \begin{equation}
        \text{Std. Error} = \sqrt{\frac{\hat{V}}{n}}. 
     \end{equation}


#### **Remark 10.3.2: Trimming**
- 傾向スコア $m_{[k]}$が 0 または 1 に近い場合、$(|\hat{H}_i|)$ が極端に大きくなる
  - これは、overlap 条件が満たされていない可能性を示唆している
- 傾向スコアの極端な値を取り除く (trimming) ことで対処
  - (\(\varepsilon = 0.01\)) し、\(\bar{H} = 100\) とすることが多い
  - 理論的・実践的にどのような trimming の値が適切か、明らかにはなっておらず、研究の発展の余地がある領域


#### **Theorem 10.3.1: Adaptive Inference on ATE with DML**
- 以下の条件を満たすと仮定する
  - Overlap 条件 (\(\varepsilon < m_0(X) < 1 - \varepsilon\)) を満たす
  - 機械学習による \(g_0, m_0\) の推定が十分に正確である。つまり、下記を満たす
  \begin{equation}
    \left\|\hat{g}_{[k]}-g_0\right\|_{L^2}+\left\|\hat{m}_{[k]}-m_0\right\|_{L^2}+\sqrt{n}\left\|_{\delta[k]}-g_0\right\|_{L^2}\left\|\hat{m}_{[k]}-m_0\right\|_{L^2} \approx 0,
  \end{equation}
  
- 局外母数 (nuisance parameter) の推定誤差は、処置効果 \(\hat{\theta}\) の推定に影響を与えない
  \begin{equation}
    \sqrt{n}\left(\hat{\theta}-\theta_0\right) \approx \sqrt{n} \mathbb{E}_n\left[\varphi_0(W)-\theta_0\right] .
  \end{equation}
  
- 推定量は $\sqrt{n}$一致性を満たし、次の分布に収束する
    \begin{equation}
      \sqrt{n}(\hat{\theta} - \theta_0) \approx \mathcal{N}(0, V), 
    \end{equation}
    ここで、
    \begin{equation}
    V = \mathbb{E}[(\varphi_0(W) - \theta_0)^2]. 
    \end{equation}
    
- $g_0$ と $m_0$ の推定にはトレードオフがある可能性があると書いてあったが、なぜ？


### DML Inference for GATEs and ATETs

#### **GATEs (Group Average Treatment Effects)**

- 特定のグループにおける平均処置効果 (GATE) を推定することもできる
\begin{equation}
  \theta_0 = \mathbb{E}[g_0(1, X) - g_0(0, X) \mid G = 1]
\end{equation}

- 例: ワクチンが与える年齢階層別の影響を知りたい
  - $G = 1$ を 13歳以上19歳以下 ($13 \leq \text{Age} \leq 19$) の若者
  - $G = 1$ を $65 \leq \text{Age}$  の高齢者

- 次の推定量で推定が可能
\begin{equation}
  \theta_0=\mathrm{E}\left[\varphi_0(X) \mid G=1\right]=\mathrm{E}\left[\varphi_0(X) G\right] / \mathrm{P}(G=1) .
\end{equation}


#### **ATET (Average Treatment Effect on the Treated)**

\begin{equation}
  \theta_0 = \mathbb{E}[g_0(1, X) - g_0(0, X) \mid D = 1]
\end{equation}


#### **Remark 10.3.3: Misspecification of PLM as inference on an overlap-weighted APE**

- PLM の仮定が成り立たない場合に、PLM の推定量 $\beta$ は何を表すか？
- IRM であるとき、$\tilde{Y}$ の $\tilde{D}$ に関する BLP (best linear predictor) がどうなるかを見るため、まず以下の式を考える
\begin{equation}
  g_0(D, X) = g_0(0, X) + D(g_0(1, X) - g_0(0, X))
\end{equation}
  - IRMは、$Y$ の非線形な予測部分と、$D$ の係数部分に分けられる
  - $g_0(0,X)$ は、PLM における $\ell(X)$ と同じ役割を果たす

- $\tilde{Y}$ は、以下のように書ける

\begin{equation}
  \tilde{Y}=\tilde{D}\left(g_0(1, X)-g_0(0, X)\right)+\epsilon .
\end{equation}

- Theorem 10.2.1 の (10.2.3) 式より、$\beta$ の定義は

\begin{equation}
  \beta:=\{b: E[(\tilde{Y}-b \tilde{D}) \tilde{D}]=0\}=\left(\mathrm{E}\left[\tilde{D}^2\right]\right)^{-1}     \mathrm{E}[\tilde{D} \tilde{Y}],
\end{equation}

- $\mathrm{E}\left[\tilde{D}^2 \mid X\right]=m_0(X)\left(1-m_0(X)\right)$ であることを利用し、$\beta$ を次のような $m_0(X)$ と $g_0(X)$ の式に書き換える

\begin{align}
  &\beta = \frac{\mathrm{E}[\tilde{D} \tilde{Y}]}{\mathrm{E}\left[\tilde{D}^2\right]} \\
  \Rightarrow &\beta = \frac{\mathrm{E}[\tilde{D} \left\{\tilde{D}\left(g_0(1, X)-g_0(0, X)\right)+\epsilon\right\}]}{\mathrm{E}\left[m_0(X)\left(1-m_0(X)\right)\right]} \quad \because \text{$\tilde{Y}$ を代入} \\
  \Rightarrow &\beta = \frac{\mathrm{E}[\tilde{D}^2\left(g_0(1, X)-g_0(0, X)\right)]}{\mathrm{E}\left[m_0(X)\left(1-m_0(X)\right)\right]} + \frac{\mathrm{E}[\tilde{D} \epsilon]}{\mathrm{E}\left[m_0(X)\left(1-m_0(X)\right)\right]} \\

  \therefore ~ &\beta = \frac{\mathbb{E}[m_0(X)(1 - m_0(X))(g_0(1, X) - g_0(0, X))]}{\mathbb{E}[m_0(X)(1 - m_0(X))]}
\end{align}

- この重みは、$m_0(X) \approx 1/2$ 付近に大きな重みを、$m_0(X) \approx 0$ または $m_0(X) \approx 1$ に小さな重みを与える
  - 傾向スコアのような、解釈のしやすい重みになっていない
  - 傾向スコア: 処置群については対照群と近いものに大きな重みを、対照群については処置群と近いものに大きな重みを与える


**＜補足: $\mathrm{E}\left[\tilde{D}^2 \mid X\right]=m_0(X)\left(1-m_0(X)\right)$ の導出＞**

- (10.3.2)式より、残差 $\tilde{D}$ は以下のように表される
\begin{align}
  &D = m_0(X) + \tilde{D}, \quad \mathbb{E}[\tilde{D} | X] = 0 \\ \tag{10.3.2} 
  \Rightarrow & \tilde{D} = D - m_0(X)
\end{align}

- $\mathbb{E}[\tilde{D}^2 \mid X]$ は
\begin{align}
  &\mathbb{E}[\tilde{D}^2 \mid X] = \mathbb{E}[(D - m_0(X))^2 \mid X] \\
  \Rightarrow &\mathbb{E}[\tilde{D}^2 \mid X] = \mathbb{E}[D^2 \mid X] - 2m_0(X)\mathbb{E}[D \mid X] + m_0(X)^2
\end{align}

- \(D \in \{0, 1\}\) であり、\(D^2 = D\) であることと \(\mathbb{E}[D \mid X] = m_0(X)\) を利用すると、
\begin{align}
  &\mathbb{E}[\tilde{D}^2 \mid X] = \mathbb{E}[D \mid X] - 2m_0(X)m_0(X) + m_0(X)^2 \\
  \Rightarrow &\mathbb{E}[\tilde{D}^2 \mid X] = m_0(X) - 2m_0(X)^2 + m_0(X)^2 \\
  \Rightarrow &\mathbb{E}[\tilde{D}^2 \mid X] = m_0(X) - m_0(X)^2 \\
  \because ~ &\mathbb{E}[\tilde{D}^2 \mid X] = m_0(X)(1 - m_0(X)).
\end{align}


#### **Remark 10.3.3: 連続な処置の場合**

- 連続的な処置 $D \in [0, 1]$ の場合も、$g_0(D, X)$ をベースライン + $D$ による効果として分解して記述できる
\begin{equation}
  g_0(D, X) = g_0(0, X) + \int_0^D t \cdot g_0'(t, X) dt
\end{equation}
- すると、$\beta$ を weighted average derivative として書ける
\begin{equation}
  \beta = \frac{\mathbb{E}[w(D, X) g_0'(D, X)]}{\mathbb{E}[w(D, X)]}
\end{equation}
ここで、重み $w(D, X)$ は次のように定義される
\begin{equation}
  w(D, X) = \frac{\mathbb{E}[\tilde{D} \mid D > d, X]}{f(D \mid X)}
\end{equation}

- binary にせよ continuous にせよ、解釈が難しいものになっている

### The Effect of 401(k) Eligibility on Net Financial Assets

- Poterba et al. (1994, 1995) の、企業型確定拠出年金制度401(k)への加入資格 が個人の金融資産に与える影響を、本節で説明してきたDMLを用いて再分析する

- **推定における課題**
  - 「401(k)のプランを提供している会社で働く」という処置は、ランダム割当ではない（選択バイアスの問題）
- **解決策**
  - 401(k) が始まった当初、労働者は401(k)が提供されているかどうかより、収入など別の側面に基づいて職業を選択している可能性が高いというアイディアを用いる
  - **条件付き独立性の仮定**: 収入や他の職業選択の要因を調整した後は、401(k)の加入資格は外生的である
- **条件付き独立性と関数型に関する議論**
  - Poterba et al. や他の研究は、事前に定義された制約の厳しい関数型を仮定しているが、正確に共変量を調整できているか？という疑問がある
  - 一方で、より柔軟なモデルを使うと、検出力が低下する
  - 本節では、両方を比べてみる

### **変数の設定とDAG**

-  **変数の定義**
   - $Y$: 個人の金融資産 (net financial assets)
   - $D$: 401(k)加入資格
   - $X$: 年齢、性別、収入、家族構成、教育年数、婚姻状況、共働きかどうか、年金加入状況、持ち家かどうか、IRA加入状況（iDeCo的な？）、など
   - $F$: 観測されない企業の属性
   - $M$: 従業員の401(k)支出額に応じて、従業員が支出する額 (employer match amount) であり、mediator になっている
   - $U$: 観測されない交絡因子


```{r, echo=FALSE, out.width="70%",  fig.align = 'center'}
knitr::include_graphics("figure/fig10.5.png")
```

- **DAGのOverview**
   - Figure 10.6: 共変量 $X$ を調整すれば因果推定が可能な構造
      - $X$ が valid adjustment set である
   - Figure 10.7: Mediator $M$ がある場合
      - 総効果は識別可能な場合がある
      - ただし、$M$ が $F$ に依存すると、$X$ だけの調整では因果推定が不十分
   - Figure 10.8: 観測されない交絡因子 $U$ が $Y$ に直接影響を与えると、因果推定は困難


### **DAG用コード**

- この部分の Replication は[小林のGitHub]()に上がっているので、各々の環境で実行可能
  - ターミナル / コマンドプロンプトで、`git clone` を実行すれば良い
- `renv` による仮想環境を使うことで、パッケージのバージョン管理を行っている
  - `renv::restore()` を行うことで、このノートブックで使用しているパッケージをインストール可能
  - `renv` パッケージをダウンロードしていない人は、最初に `install.packages("renv")` を実行する必要がある
- パッケージの読み込みには `pacman::p_load()` がおすすめ
  - 未インストールのパッケージを自動でインストールしてくれる


```{r}
# install.packages("renv")
# renv::restore()
# remotes::install_github("mlr-org/mlr3extralearners", force = TRUE) # needed to run boosting

pacman::p_load(
  dagitty,
  ggdag,
  xtable,
  hdm,
  sandwich,
  ggplot2,
  randomForest,
  data.table,
  glmnet,
  rpart,
  gbm,
  DoubleML, 
  mlr3learners, 
  mlr3, 
  data.table, 
  randomForest, 
  ranger,
  mlr3extralearners,
  mboost
)
```


---

#### **Figure 10.6: $X$ を調整すれば因果推定が可能な構造**


```{r}
# generate a DAGs and plot them

G1 <- dagitty('dag{
Y [outcome,pos="4, 0"]
D [exposure,pos="0, 0"]
X [confounder, pos="2,-1"]
F [uobserved, pos="0, -1"]
D -> Y
X -> D
F -> X
F -> D
X -> Y}')

G1_dag <- ggdag(G1) + theme_dag()

G1_dag$layers[[3]]$mapping <- 
  aes(colour = c("Observed", "Unobserved")[as.numeric(name == "F") + 1])
G1_dag + scale_color_manual(values = c("black", "blue")) +
  theme(legend.position.inside = c(0.8, 0.8))
```


```{r}
adjustmentSets(G1, "D", "Y", effect = "total")
```

```{r}
# generate a couple of DAGs and plot them

G2 <- dagitty('dag{
Y [outcome,pos="4, 0"]
D [exposure,pos="0, 0"]
X [confounder, pos="2,-1"]
F [uobserved, pos="0, -1"]
D -> Y
X -> D
X -> F
F -> D
X -> Y}')


G2_dag <- ggdag(G2) + theme_dag()

G2_dag$layers[[3]]$mapping <- 
  aes(colour = c("Observed", "Unobserved")[as.numeric(name == "F") + 1])
G2_dag + scale_color_manual(values = c("black", "blue")) +
  theme(legend.position.inside = c(0.8, 0.8))
```

```{r}
adjustmentSets(G2, "D", "Y", effect = "total")
```

```{r}
G3 <- dagitty('dag{
Y [outcome,pos="4, 0"]
D [exposure,pos="0, 0"]
X [confounder, pos="2,-1"]
F [unobserved, pos="0, -1"]
U [unobserved, pos="2, -2"]
D -> Y
X -> D
F -> D
U -> F
U -> X
U -> D
X -> Y}')

G3_dag <- ggdag(G3) + theme_dag()

G3_dag$layers[[3]]$mapping <- 
  aes(colour = c("Observed", "Unobserved")[as.numeric(name %in% c("F","U")) + 1])
G3_dag + scale_color_manual(values = c("black", "blue")) +
  theme(legend.position.inside = c(0.8, 0.8))

adjustmentSets(G3, "D", "Y", effect = "total")

```

- **$F$ が直接アウトカム $Y$ に影響を与える場合、識別はできない**

```{r}
G4 <- dagitty('dag{
Y [outcome,pos="4, 0"]
D [exposure,pos="0, 0"]
X [confounder, pos="2,-1"]
F [unobserved, pos="0, -1"]
U [unobserved, pos="2, -2"]
D -> Y
X -> D
F -> D
U -> F
U -> X
U -> D
F -> Y
X -> Y}')


G4_dag <- ggdag(G4) + theme_dag()

G4_dag$layers[[3]]$mapping <- 
  aes(colour = c("Observed", "Unobserved")[as.numeric(name %in% c("F","U")) + 1])
G4_dag + scale_color_manual(values = c("black", "blue")) +
  theme(legend.position.inside = c(0.8, 0.8))

adjustmentSets(G4, "D", "Y", effect = "total")
```

---

#### **Figure 10.7: mediator  $M$ がある場合**

```{r}
G5 <- dagitty('dag{
Y [outcome,pos="4, 0"]
D [exposure,pos="0, 0"]
X [confounder, pos="2,-2"]
F [unobserved, pos="0, -2"]
U [unobserved, pos="2, -3"]
M [unobserved, pos="3, -.1"]
D -> Y
X -> D
F -> D
U -> F
U -> X
U -> D
D -> M
M -> Y
X -> M
X -> Y}')

G5_dag <- ggdag(G5) + theme_dag()

G5_dag$layers[[3]]$mapping <- 
  aes(colour = c("Observed", "Unobserved")[as.numeric(name %in% c("F","U", "M")) + 1])
G5_dag + scale_color_manual(values = c("black", "blue")) +
  theme(legend.position.inside = c(0.8, 0.8))

print(adjustmentSets(G5, "D", "Y", effect = "total"))

```

- **$F$ が直接 Mediator $M$ に影響を与える場合、識別はできない**


```{r}
G6 <- dagitty('dag{
Y [outcome,pos="4, 0"]
D [exposure,pos="0, 0"]
X [confounder, pos="2,-2"]
F [unobserved, pos="0, -2"]
U [unobserved, pos="2, -3"]
M [unobserved, pos="3, -.1"]
D -> Y
X -> D
F -> D
U -> F
U -> X
D -> M
F -> M
U -> D
M -> Y
X -> M
X -> Y}')

G6_dag <- ggdag(G6) + theme_dag()

G6_dag$layers[[3]]$mapping <- 
  aes(colour = c("Observed", "Unobserved")[as.numeric(name %in% c("F","U", "M")) + 1])
G6_dag + scale_color_manual(values = c("black", "blue")) +
  theme(legend.position.inside = c(0.8, 0.8))

print(adjustmentSets(G6, "D", "Y"), effect = "total")
```

**観測されない交絡因子$U$が直接アウトカム$Y$に影響を与える場合、識別はできない**

```{r}
G7<- dagitty('dag{
Y [outcome,pos="4, 0"]
D [exposure,pos="0, 0"]
X [confounder, pos="2,-2"]
F [unobserved, pos="0, -2"]
U [unobserved, pos="2, -3"]
M [unobserved, pos="3, -.1"]
D -> Y
X -> D
F -> D
U -> F
U -> X
D -> M
U -> D
M -> Y
X -> M
X -> Y
U -> Y}')

G7_dag <- ggdag(G7) + theme_dag()

G7_dag$layers[[3]]$mapping <- 
  aes(colour = c("Observed", "Unobserved")[as.numeric(name %in% c("F","U", "M")) + 1])
G7_dag + scale_color_manual(values = c("black", "blue")) +
  theme(legend.position.inside = c(0.8, 0.8))

print(adjustmentSets(G7, "D", "Y"), effect = "total")
```

---

### **推定コードと結果**

#### **Data**

- `hdm` パッケージに含まれており、ダウンロード可能である

```{r}
data(pension)
data <- pension
dim(data)
```


```{r}
help(pension)
```


```{r}
hist_e401 <- ggplot(data, aes(x = e401, fill = factor(e401))) +
  geom_bar()
hist_e401
```


```{r}
dens_net_tfa <- ggplot(data, aes(x = net_tfa, color = factor(e401), fill = factor(e401))) +
  geom_density() +
  xlim(c(-20000, 150000)) +
  facet_wrap(. ~ e401)

dens_net_tfa
```


```{r}
e1 <- data[data$e401 == 1, ]
e0 <- data[data$e401 == 0, ]
round(mean(e1$net_tfa) - mean(e0$net_tfa), 0)
```


```{r}
p1 <- data[data$p401 == 1, ]
p0 <- data[data$p401 == 0, ]
round(mean(p1$net_tfa) - mean(p0$net_tfa), 0)
```

```{r}
# outcome variable
y <- data[, "net_tfa"]
# treatment variable
D <- data[, "e401"]
D2 <- data[, "p401"]
D3 <- data[, "a401"]

columns_to_drop <- c(
  "e401", "p401", "a401", "tw", "tfa", "net_tfa", "tfa_he",
  "hval", "hmort", "hequity",
  "nifa", "net_nifa", "net_n401", "ira",
  "dum91", "icat", "ecat", "zhat",
  "i1", "i2", "i3", "i4", "i5", "i6", "i7",
  "a1", "a2", "a3", "a4", "a5"
)

# covariates
X <- data[, !(names(data) %in% columns_to_drop)]
```

```{r}
# Constructing the controls
x_formula <- paste("~ 0 + poly(age, 6, raw=TRUE) + poly(inc, 8, raw=TRUE) + poly(educ, 4, raw=TRUE) ",
                   "+ poly(fsize, 2, raw=TRUE) + male + marr + twoearn + db + pira + hown")
X <- as.data.table(model.frame(x_formula, X))
head(X)
```

---

#### **Partiall Linear Model (PLM)**

```{r}
dml2_for_plm <- function(x, d, y, dreg, yreg, nfold = 3, method = "regression") {
  nobs <- nrow(x) # number of observations
  foldid <- rep.int(1:nfold, times = ceiling(nobs / nfold))[sample.int(nobs)] # define folds indices
  I <- split(1:nobs, foldid) # split observation indices into folds
  ytil <- dtil <- rep(NA, nobs)
  cat("fold: ")
  for (b in seq_along(I)) {
    if (method == "regression") {
      dfit <- dreg(x[-I[[b]], ], d[-I[[b]]]) # take a fold out
      yfit <- yreg(x[-I[[b]], ], y[-I[[b]]]) # take a foldt out
      dhat <- predict(dfit, x[I[[b]], ], type = "response") # predict the left-out fold
      yhat <- predict(yfit, x[I[[b]], ], type = "response") # predict the left-out fold
      dtil[I[[b]]] <- (d[I[[b]]] - dhat) # record residual for the left-out fold
      ytil[I[[b]]] <- (y[I[[b]]] - yhat) # record residial for the left-out fold
    } else if (method == "randomforest") {
      dfit <- dreg(x[-I[[b]], ], as.factor(d)[-I[[b]]]) # take a fold out
      yfit <- yreg(x[-I[[b]], ], y[-I[[b]]]) # take a fold out
      dhat <- predict(dfit, x[I[[b]], ], type = "prob")[, 2] # predict the left-out fold
      yhat <- predict(yfit, x[I[[b]], ], type = "response") # predict the left-out fold
      dtil[I[[b]]] <- (d[I[[b]]] - dhat) # record residual for the left-out fold
      ytil[I[[b]]] <- (y[I[[b]]] - yhat) # record residial for the left-out fold
    } else if (method == "decisiontrees") {
      dfit <- dreg(x[-I[[b]], ], as.factor(d)[-I[[b]]]) # take a fold out
      yfit <- yreg(x[-I[[b]], ], y[-I[[b]]]) # take a fold out
      dhat <- predict(dfit, x[I[[b]], ])[, 2] # predict the left-out fold
      yhat <- predict(yfit, x[I[[b]], ]) # predict the left-out fold
      dtil[I[[b]]] <- (d[I[[b]]] - dhat) # record residual for the left-out fold
      ytil[I[[b]]] <- (y[I[[b]]] - yhat) # record residial for the left-out fold
    } else if (method == "boostedtrees") {
      dfit <- dreg(x[-I[[b]], ], d[-I[[b]]]) # take a fold out
      yfit <- yreg(x[-I[[b]], ], y[-I[[b]]]) # take a fold out
      dhat <- predict(dfit, x[I[[b]], ], type = "response") # predict the left-out fold
      yhat <- predict(yfit, x[I[[b]], ], type = "response") # predict the left-out fold
      dtil[I[[b]]] <- (d[I[[b]]] - dhat) # record residual for the left-out fold
      ytil[I[[b]]] <- (y[I[[b]]] - yhat) # record residial for the left-out fold
    }
    cat(b, " ")
  }
  rfit <- lm(ytil ~ dtil) # estimate the main parameter by regressing one residual on the other
  coef_est <- coef(rfit)[2] # extract coefficient
  se <- sqrt(vcovHC(rfit)[2, 2]) # record robust standard error
  cat(sprintf("\ncoef (se) = %g (%g)\n", coef_est, se)) # printing output
  return(list(coef_est = coef_est, se = se, dtil = dtil, ytil = ytil)) # save output and residuals
}
```


```{r}
summaryPLR <- function(point, stderr, resD, resy, name) {
  data <- data.frame(
    estimate = point, # point estimate
    stderr = stderr, # standard error
    lower = point - 1.96 * stderr, # lower end of 95% confidence interval
    upper = point + 1.96 * stderr, # upper end of 95% confidence interval
    `rmse y` = sqrt(mean(resy^2)), # RMSE of model that predicts outcome y
    `rmse D` = sqrt(mean(resD^2)), # RMSE of model that predicts treatment D
    `accuracy D` = mean(abs(resD) < 0.5) # binary classification accuracy of model for D
  )
  rownames(data) <- name
  return(data)
}
```

**Double Lasso**

- 処置の推定において、関数型を線形確率モデルにする場合

```{r}
# DML with LassoCV
set.seed(123)
cat(sprintf("\nDML with Lasso CV \n"))

dreg_lasso_cv <- function(x, d) {
  cv.glmnet(x, d, family = "gaussian", alpha = 1, nfolds = 5)
}
yreg_lasso_cv <- function(x, y) {
  cv.glmnet(x, y, family = "gaussian", alpha = 1, nfolds = 5)
}

dml2_results <- dml2_for_plm(as.matrix(X), D, y, dreg_lasso_cv, yreg_lasso_cv, nfold = 5)

sum_lasso_cv <- summaryPLR(dml2_results$coef_est, dml2_results$se, dml2_results$dtil,
                           dml2_results$ytil, name = "LassoCV")
tableplr <- data.frame()
tableplr <- rbind(sum_lasso_cv)
tableplr
``` 

```{r}
# Because residuals are output, reconstruct fitted values for use in ensemble
dhat_lasso <- D - dml2_results$dtil
yhat_lasso <- y - dml2_results$ytil
```

- 処置の推定において、Logistic 回帰を用いる場合

```{r}
# DML with Lasso/Logistic
set.seed(123)
cat(sprintf("\nDML with Lasso/Logistic \n"))

dreg_logistic_cv <- function(x, d) {
  cv.glmnet(x, d, family = "binomial", alpha = 0, nfolds = 5)
}
yreg_lasso_cv <- function(x, y) {
  cv.glmnet(x, y, family = "gaussian", alpha = 1, nfolds = 5)
}

dml2_results <- dml2_for_plm(as.matrix(X), D, y, dreg_logistic_cv, yreg_lasso_cv, nfold = 5)
sum_lasso_logistic_cv <- summaryPLR(dml2_results$coef_est, dml2_results$se, dml2_results$dtil,
                                    dml2_results$ytil, name = "LassoCV/LogisticCV")
tableplr <- rbind(tableplr, sum_lasso_logistic_cv)
tableplr
```

```{r}
# Because residuals are output, reconstruct fitted values for use in ensemble
dhat_lasso_logistic <- D - dml2_results$dtil
yhat_lasso_logistic <- y - dml2_results$ytil
```


**Random Forest**

```{r}
# DML with Random Forest
set.seed(123)
cat(sprintf("\nDML with Random Forest \n"))

dreg_rf <- function(x, d) {
  randomForest(x, d, ntree = 1000, nodesize = 10)
} # ML method=Forest
yreg_rf <- function(x, y) {
  randomForest(x, y, ntree = 1000, nodesize = 10)
} # ML method=Forest

dml2_results <- dml2_for_plm(as.matrix(X), D, y, dreg_rf, yreg_rf, nfold = 5, method = "randomforest")
sum_rf <- summaryPLR(dml2_results$coef_est, dml2_results$se, dml2_results$dtil,
                     dml2_results$ytil, name = "Random Forest")
tableplr <- rbind(tableplr, sum_rf)
tableplr
```



```{r}
# Because residuals are output, reconstruct fitted values for use in ensemble
dhat_rf <- D - dml2_results$dtil
dhat_rf <- y - dml2_results$ytil
```

**Decision Trees**

```{r}
# DML with Decision Trees
set.seed(123)
cat(sprintf("\nDML with Decision Trees \n"))

dreg_tr <- function(x, d) {
  rpart(as.formula("D~."), cbind(data.frame(D = d), x), method = "class", minbucket = 10, cp = 0.001)
}
dreg_tr <- function(x, y) {
  rpart(as.formula("y~."), cbind(data.frame(y = y), x), minbucket = 10, cp = 0.001)
}

# decision tree takes in X as dataframe, not matrix/array
dml2_results <- dml2_for_plm(X, D, y, dreg_tr, dreg_tr, nfold = 5, method = "decisiontrees")
sum_tr <- summaryPLR(dml2_results$coef_est, dml2_results$se, dml2_results$dtil,
                     dml2_results$ytil, name = "Decision Trees")
tableplr <- rbind(tableplr, sum_tr)
tableplr
```

```{r}
# Because residuals are output, reconstruct fitted values for use in ensemble
dhat_tr <- D - dml2_results$dtil
yhat_tr <- y - dml2_results$ytil
```

**Boosted Trees**

```{r}
# DML with Boosted Trees
set.seed(123)
cat(sprintf("\nDML with Boosted Trees \n"))

# NB: early stopping cannot easily be implemented with gbm
## set n.trees = best, where best <- gbm.perf(dreg_boost, plot.it = FALSE)
dreg_boost <- function(x, d) {
  gbm(as.formula("D~."), cbind(data.frame(D = d), x), distribution = "bernoulli",
      interaction.depth = 2, n.trees = 100, shrinkage = .1)
}
yreg_boost <- function(x, y) {
  gbm(as.formula("y~."), cbind(data.frame(y = y), x), distribution = "gaussian",
      interaction.depth = 2, n.trees = 100, shrinkage = .1)
}

# passing these through regression as type="response", and D should not be factor!
dml2_results <- dml2_for_plm(X, D, y, dreg_boost, yreg_boost, nfold = 5, method = "boostedtrees")
sum_boost <- summaryPLR(dml2_results$coef_est, dml2_results$se, dml2_results$dtil,
                        dml2_results$ytil, name = "Boosted Trees")
tableplr <- rbind(tableplr, sum_boost)
tableplr
```

```{r}
# Because residuals are output, reconstruct fitted values for use in ensemble
dhat_boost <- D - dml2_results$dtil
yhat_boost <- y - dml2_results$ytil
```

**Ensemble**

```{r}
# Best fit is boosted trees for both D and Y

sum_best <- summaryPLR(dml2_results$coef_est, dml2_results$se, dml2_results$dtil,
                       dml2_results$ytil, name = "Best")
tableplr <- rbind(tableplr, sum_best)
tableplr
```

```{r}
# Least squares model average

ma_dtil <- lm(D ~ dhat_lasso + dhat_lasso_logistic + dhat_rf + dhat_tr + dhat_boost)$residuals
ma_ytil <- lm(y ~ yhat_lasso + yhat_lasso_logistic + dhat_rf + yhat_tr + yhat_boost)$residuals

rfit <- lm(ma_ytil ~ ma_dtil) # estimate the main parameter by regressing one residual on the other
coef_est <- coef(rfit)[2] # extract coefficient
se <- sqrt(vcovHC(rfit)[2, 2]) # record robust standard error

sum.ma <- summaryPLR(coef_est, se, ma_dtil, ma_ytil, name = "Model Average")
tableplr <- rbind(tableplr, sum.ma)
tableplr
```

---
#### **Interactive Regression Model (IRM)**

```{r}
dml2_for_irm <- function(x, d, y, dreg, yreg0, yreg1, trimming = 0.01, nfold = 5, method = "regression") {
  yhat0 <- rep(0, length(y))
  yhat1 <- rep(0, length(y))
  Dhat <- rep(0, length(d))

  nobs <- nrow(x) # number of observations
  foldid <- rep.int(1:nfold, times = ceiling(nobs / nfold))[sample.int(nobs)] # define folds indices
  I <- split(1:nobs, foldid) # split observation indices into folds
  ytil <- dtil <- rep(NA, nobs)

  cat("fold: ")
  for (b in seq_along(I)) {
    # define helpful variables
    Dnotb <- d[-I[[b]]]
    Xb <- X[I[[b]], ]
    Xnotb <- X[-I[[b]], ]

    # training dfs subsetted on the -I[[b]] fold
    XD0 <- X[-I[[b]], ][d[-I[[b]]] == 0]
    yD0 <- y[-I[[b]]][d[-I[[b]]] == 0]
    XD1 <- X[-I[[b]], ][d[-I[[b]]] == 1]
    yD1 <- y[-I[[b]]][d[-I[[b]]] == 1]

    if (method == "regression") {
      yfit0 <- yreg0(as.matrix(XD0), yD0)
      yfit1 <- yreg1(as.matrix(XD1), yD1)
      yhat0[I[[b]]] <- predict(yfit0, as.matrix(Xb)) # default is type = "response" for glmnet family gaussian
      yhat1[I[[b]]] <- predict(yfit1, as.matrix(Xb))
    } else if (method == "randomforest") {
      yfit0 <- yreg0(XD0, yD0)
      yfit1 <- yreg1(XD1, yD1)
      yhat0[I[[b]]] <- predict(yfit0, Xb) # default is type = "response" for rf
      yhat1[I[[b]]] <- predict(yfit1, Xb)
    } else if (method == "decisiontrees") {
      yfit0 <- yreg0(XD0, yD0)
      yfit1 <- yreg1(XD1, yD1)
      yhat0[I[[b]]] <- predict(yfit0, Xb) # default is type = "vector" for decision
      yhat1[I[[b]]] <- predict(yfit1, Xb)
    } else if (method == "boostedtrees") {
      yfit0 <- yreg0(as.data.frame(XD0), yD0)
      yfit1 <- yreg1(as.data.frame(XD1), yD1)
      yhat0[I[[b]]] <- predict(yfit0, Xb) # default is type = "response" for boosted
      yhat1[I[[b]]] <- predict(yfit1, Xb)
    }

    # propensity scores:
    if (method == "regression") {
      dfit_b <- dreg(as.matrix(Xnotb), Dnotb)
      dhat_b <- predict(dfit_b, as.matrix(Xb), type = "response") # default is type="link" for family binomial!
    } else if (method == "randomforest") {
      dfit_b <- dreg(Xnotb, as.factor(Dnotb))
      dhat_b <- predict(dfit_b, Xb, type = "prob")[, 2]
    } else if (method == "decisiontrees") {
      dfit_b <- dreg(Xnotb, Dnotb)
      dhat_b <- predict(dfit_b, Xb)[, 2]
    } else if (method == "boostedtrees") {
      dfit_b <- dreg(as.data.frame(Xnotb), Dnotb)
      dhat_b <- predict(dfit_b, Xb, type = "response")
    }
    dhat_b <- pmax(pmin(dhat_b, 1 - trimming), trimming) # trimming so scores are between [trimming, (1-trimming)]
    Dhat[I[[b]]] <- dhat_b

    cat(b, " ")
  }

  # Prediction of treatment and outcome for observed instrument
  yhat <- yhat0 * (1 - D) + yhat1 * D
  # residuals
  ytil <- y - yhat
  dtil <- D - Dhat
  # doubly robust quantity for every sample
  drhat <- yhat1 - yhat0 + (y - yhat) * (D / Dhat - (1 - D) / (1 - Dhat))
  coef_est <- mean(drhat)
  vari <- var(drhat)
  se <- sqrt(vari / nrow(X))
  cat("point", coef_est)
  cat("se", se)
  return(list(coef_est = coef_est, se = se, ytil = ytil, dtil = dtil, drhat = drhat,
              yhat0 = yhat0, yhat1 = yhat1, dhat = Dhat, yhat = yhat))
}

summaryIRM <- function(coef_est, se, ytil, dtil, drhat, name) {
  summary_data <- data.frame(
    estimate = coef_est, # point estimate
    se = se, # standard error
    lower = coef_est - 1.96 * se, # lower end of 95% confidence interval
    upper = coef_est + 1.96 * se, # upper end of 95% confidence interval
    rmse_y = sqrt(mean(ytil^2)), # res of model that predicts outcome y
    rmse_D = sqrt(mean(dtil^2)), # res of model that predicts treatment D
    accuracy_D = mean(abs(dtil) < 0.5) # binary classification accuracy of model for D
  )
  row.names(summary_data) <- name
  return(summary_data)
}
```

**Double Lasso**

```{r}
# DML with Lasso/Logistic
set.seed(123)
cat(sprintf("\nDML with LassoCV/Logistic \n"))

dreg_lasso_cv <- function(x, d) {
  cv.glmnet(x, d, family = "binomial", alpha = 0, nfolds = 5)
}
yreg0_lasso_cv <- function(x, y) {
  cv.glmnet(x, y, family = "gaussian", alpha = 1, nfolds = 5)
}
yreg1_lasso_cv <- function(x, y) {
  cv.glmnet(x, y, family = "gaussian", alpha = 1, nfolds = 5)
}

# more folds seems to help stabilize finite sample performance
dml2_results <- dml2_for_irm(X, D, y, dreg_lasso_cv, yreg0_lasso_cv, yreg1_lasso_cv, nfold = 5)
sum_lasso_cv <- summaryIRM(dml2_results$coef_est, dml2_results$se, dml2_results$ytil, dml2_results$dtil,
                           dml2_results$drhat, name = "LassoCVLogistic")
tableirm <- data.frame()
tableirm <- rbind(sum_lasso_cv)
tableirm

yhat0_lasso <- dml2_results$yhat0
yhat1_lasso <- dml2_results$yhat1
dhat_lasso <- dml2_results$dhat
yhat_lasso <- dml2_results$yhat
```

**Random Forest**

```{r}
# DML with Random Forest
set.seed(123)
cat(sprintf("\nDML with Random Forest \n"))

dreg_rf <- function(x, d) {
  randomForest(x, d, ntree = 1000, nodesize = 10)
} # ML method=Forest
yreg0_rf <- function(x, y) {
  randomForest(x, y, ntree = 1000, nodesize = 10)
} # ML method=Forest
yreg1_rf <- function(x, y) {
  randomForest(x, y, ntree = 1000, nodesize = 10)
} # ML method=Forest


dml2_results <- dml2_for_irm(as.matrix(X), D, y, dreg_rf, yreg0_rf, yreg1_rf, nfold = 5, method = "randomforest")
sum_rf <- summaryIRM(dml2_results$coef_est, dml2_results$se, dml2_results$ytil, dml2_results$dtil,
                     dml2_results$drhat, name = "Random Forest")
tableirm <- rbind(tableirm, sum_rf)
tableirm

yhat0_rf <- dml2_results$yhat0
yhat1_rf <- dml2_results$yhat1
dhat_rf <- dml2_results$dhat
dhat_rf <- dml2_results$yhat
```

**Decision Trees**

```{r}
# DML with Decision Trees
set.seed(123)
cat(sprintf("\nDML with Decision Trees \n"))

dreg_tr <- function(x, d) {
  rpart(as.formula("D~."), cbind(data.frame(D = d), x), method = "class", minbucket = 10, cp = 0.001)
}
yreg0_tr <- function(x, y) {
  rpart(as.formula("y~."), cbind(data.frame(y = y), x), minbucket = 10, cp = 0.001)
}
yreg1_tr <- function(x, y) {
  rpart(as.formula("y~."), cbind(data.frame(y = y), x), minbucket = 10, cp = 0.001)
}

dml2_results <- dml2_for_irm(X, D, y, dreg_tr, yreg0_tr, yreg1_tr, nfold = 5, method = "decisiontrees")
sum_tr <- summaryIRM(dml2_results$coef_est, dml2_results$se, dml2_results$ytil, dml2_results$dtil,
                     dml2_results$drhat, name = "Decision Trees")
tableirm <- rbind(tableirm, sum_tr)
tableirm

yhat0_tr <- dml2_results$yhat0
yhat1_tr <- dml2_results$yhat1
dhat_tr <- dml2_results$dhat
yhat_tr <- dml2_results$yhat
```

**Boosted Trees**

```{r}
# DML with Boosted Trees
set.seed(123)
cat(sprintf("\nDML with Boosted Trees \n"))

# NB: early stopping cannot easily be implemented with gbm
## set n.trees = best, where best <- gbm.perf(dreg_boost, plot.it = FALSE)
dreg_boost <- function(x, d) {
  gbm(as.formula("D~."), cbind(data.frame(D = d), x), distribution = "bernoulli",
      interaction.depth = 2, n.trees = 100, shrinkage = .1)
}
yreg0_boost <- function(x, y) {
  gbm(as.formula("y~."), cbind(data.frame(y = y), x), distribution = "gaussian",
      interaction.depth = 2, n.trees = 100, shrinkage = .1)
}
yreg1_boost <- function(x, y) {
  gbm(as.formula("y~."), cbind(data.frame(y = y), x), distribution = "gaussian",
      interaction.depth = 2, n.trees = 100, shrinkage = .1)
}

# passing these through regression as type="response", and D should not be factor!
dml2_results <- dml2_for_irm(X, D, y, dreg_boost, yreg0_boost, yreg1_boost, nfold = 5, method = "boostedtrees")
sum_boost <- summaryIRM(dml2_results$coef_est, dml2_results$se, dml2_results$ytil, dml2_results$dtil,
                        dml2_results$drhat, name = "Boosted Trees")
tableirm <- rbind(tableirm, sum_boost)
tableirm

yhat0_boost <- dml2_results$yhat0
yhat1_boost <- dml2_results$yhat1
dhat_boost <- dml2_results$dhat
yhat_boost <- dml2_results$yhat
```

**Ensemble**

```{r}
# Ensembles

# Best
# We'll look at model that does best for Y overall. Could also use different model for Y0 and Y1
# Here, the best performance for Y is the random forest and for D the boosted tree

# residuals
ytil <- y - dhat_rf
dtil <- D - dhat_boost
# doubly robust quantity for every sample
drhat <- yhat1_rf - yhat0_rf + (y - dhat_rf) * (D / dhat_boost - (1 - D) / (1 - dhat_boost))
coef_est <- mean(drhat)
vari <- var(drhat)
se <- sqrt(vari / nrow(X))

sum_best <- summaryIRM(coef_est, se, ytil, dtil, drhat, name = "Best")
tableirm <- rbind(tableirm, sum_best)
tableirm
```

```{r}
# Least squares model average
# We'll look at weights that do best job for Y overall. Could also use different weights for Y0 and Y1

ma_dw <- lm(D ~ dhat_lasso + dhat_rf + dhat_tr + dhat_boost)$coef
ma_yw <- lm(y ~ yhat_lasso + dhat_rf + yhat_tr + yhat_boost)$coef

Dhats <- cbind(as.matrix(rep(1, nrow(X))), dhat_lasso, dhat_rf, dhat_tr, dhat_boost)
Y0s <- cbind(as.matrix(rep(1, nrow(X))), yhat0_lasso, yhat0_rf, yhat0_tr, yhat0_boost)
Y1s <- cbind(as.matrix(rep(1, nrow(X))), yhat1_lasso, yhat1_rf, yhat1_tr, yhat1_boost)

dhat <- Dhats %*% as.matrix(ma_dw)
yhat0 <- Y0s %*% as.matrix(ma_yw)
yhat1 <- Y1s %*% as.matrix(ma_yw)

# Prediction of treatment and outcome for observed instrument
yhat <- yhat0 * (1 - D) + yhat1 * D
# residuals
ytil <- y - yhat
dtil <- D - dhat
# doubly robust quantity for every sample
drhat <- yhat1 - yhat0 + (y - yhat) * (D / dhat - (1 - D) / (1 - dhat))
coef_est <- mean(drhat)
vari <- var(drhat)
se <- sqrt(vari / nrow(X))

sum.ma <- summaryIRM(coef_est, se, ytil, dtil, drhat, name = "Model Average")
tableirm <- rbind(tableirm, sum.ma)
tableirm
```

---

### DoubleML package による推定

- 今までは自ら関数を書いていたが、`EconML` (Python) や `DoubleML` (R & Python) などのパッケージが利用可能である
  - [DoubleML パッケージのサイト](https://docs.doubleml.org/stable/index.html)
  - [ml3 パッケージ](https://mlr3book.mlr-org.com) を内部で動かしている

**データ作成**

```{r}
# Constructing the data (as DoubleMLData)
formula_flex <- paste("net_tfa ~ e401 + poly(age, 6, raw=TRUE) + poly(inc, 8, raw=TRUE) ",
                      "+ poly(educ, 4, raw=TRUE) + poly(fsize, 2, raw=TRUE) + marr + twoearn + db + pira + hown")
model_flex <- as.data.table(model.frame(formula_flex, pension))
x_cols <- colnames(model_flex)[-c(1, 2)]
data_ml <- DoubleMLData$new(model_flex, y_col = "net_tfa", d_cols = "e401", x_cols = x_cols)

p <- dim(model_flex)[2] - 2
p
```

#### **Partiall Linear Model (PLM)**

```{r}
# Estimating the PLR
lgr::get_logger("mlr3")$set_threshold("warn")
lasso <- lrn("regr.cv_glmnet", nfolds = 5, s = "lambda.min")
lasso_class <- lrn("classif.cv_glmnet", nfolds = 5, s = "lambda.min")

dml_plr <- DoubleMLPLR$new(data_ml, ml_l = lasso, ml_m = lasso_class, n_folds = 5)
dml_plr$fit(store_predictions = TRUE)
dml_plr$summary()
lasso_plr <- dml_plr$coef
lasso_std_plr <- dml_plr$se
```

```{r}
dml_plr$params_names()
g_hat <- as.matrix(dml_plr$predictions$ml_l) # predictions of g_o
m_hat <- as.matrix(dml_plr$predictions$ml_m) # predictions of m_o
```

```{r}
# cross-fitted RMSE: outcome
y <- as.matrix(pension$net_tfa) # true observations
theta <- as.numeric(dml_plr$coef) # estimated regression coefficient
d <- as.matrix(pension$e401)
predictions_y <- as.matrix(d * theta) + g_hat # predictions for y
lasso_y_rmse <- sqrt(mean((y - predictions_y)^2))
lasso_y_rmse
```

**Lasso**

```{r}
# cross-fitted RMSE: treatment
d <- as.matrix(pension$e401)
lasso_d_rmse <- sqrt(mean((d - m_hat)^2))
lasso_d_rmse

# cross-fitted ce: treatment
mean(ifelse(m_hat > 0.5, 1, 0) != d)
```

**Random Forest**
```{r}
# Random Forest
lgr::get_logger("mlr3")$set_threshold("warn")
randomForest <- lrn("regr.ranger")
random_forest_class <- lrn("classif.ranger")

dml_plr <- DoubleMLPLR$new(data_ml, ml_l = randomForest, ml_m = random_forest_class, n_folds = 5)
dml_plr$fit(store_predictions = TRUE) # set store_predictions=TRUE to evaluate the model
dml_plr$summary()
forest_plr <- dml_plr$coef
forest_std_plr <- dml_plr$se
```

```{r}
# Evaluation predictions
g_hat <- as.matrix(dml_plr$predictions$ml_l) # predictions of g_o
m_hat <- as.matrix(dml_plr$predictions$ml_m) # predictions of m_o
theta <- as.numeric(dml_plr$coef) # estimated regression coefficient
predictions_y <- as.matrix(d * theta) + g_hat # predictions for y
forest_y_rmse <- sqrt(mean((y - predictions_y)^2))
forest_y_rmse

# cross-fitted RMSE: treatment
forest_d_rmse <- sqrt(mean((d - m_hat)^2))
forest_d_rmse

# cross-fitted ce: treatment
mean(ifelse(m_hat > 0.5, 1, 0) != d)
```

**Decision Trees**


```{r}
# Trees
lgr::get_logger("mlr3")$set_threshold("warn")

trees <- lrn("regr.rpart")
trees_class <- lrn("classif.rpart")

dml_plr <- DoubleMLPLR$new(data_ml, ml_l = trees, ml_m = trees_class, n_folds = 5)
dml_plr$fit(store_predictions = TRUE)
dml_plr$summary()
tree_plr <- dml_plr$coef
tree_std_plr <- dml_plr$se

# Evaluation predictions
g_hat <- as.matrix(dml_plr$predictions$ml_l) # predictions of g_o
m_hat <- as.matrix(dml_plr$predictions$ml_m) # predictions of m_o
theta <- as.numeric(dml_plr$coef) # estimated regression coefficient
predictions_y <- as.matrix(d * theta) + g_hat # predictions for y
tree_y_rmse <- sqrt(mean((y - predictions_y)^2))
tree_y_rmse

# cross-fitted RMSE: treatment
tree_d_rmse <- sqrt(mean((d - m_hat)^2))
tree_d_rmse

# cross-fitted ce: treatment
mean(ifelse(m_hat > 0.5, 1, 0) != d)
```

**Boosted Trees**

```{r}
# Boosting
boost <- lrn("regr.glmboost")
boost_class <- lrn("classif.glmboost")

dml_plr <- DoubleMLPLR$new(data_ml, ml_l = boost, ml_m = boost_class, n_folds = 5)
dml_plr$fit(store_predictions = TRUE)
dml_plr$summary()
boost_plr <- dml_plr$coef
boost_std_plr <- dml_plr$se

# Evaluation predictions
g_hat <- as.matrix(dml_plr$predictions$ml_l) # predictions of g_o
m_hat <- as.matrix(dml_plr$predictions$ml_m) # predictions of m_o
theta <- as.numeric(dml_plr$coef) # estimated regression coefficient
predictions_y <- as.matrix(d * theta) + g_hat # predictions for y
boost_y_rmse <- sqrt(mean((y - predictions_y)^2))
boost_y_rmse

# cross-fitted RMSE: treatment
boost_d_rmse <- sqrt(mean((d - m_hat)^2))
boost_d_rmse

# cross-fitted ce: treatment
mean(ifelse(m_hat > 0.5, 1, 0) != d)
```

**まとめ**

```{r}
table <- matrix(0, 4, 4)
table[1, 1:4] <- c(lasso_plr, forest_plr, tree_plr, boost_plr)
table[2, 1:4] <- c(lasso_std_plr, forest_std_plr, tree_std_plr, boost_std_plr)
table[3, 1:4] <- c(lasso_y_rmse, forest_y_rmse, tree_y_rmse, boost_y_rmse)
table[4, 1:4] <- c(lasso_d_rmse, forest_d_rmse, tree_d_rmse, boost_d_rmse)
rownames(table) <- c("Estimate", "Std.Error", "RMSE Y", "RMSE D")
colnames(table) <- c("Lasso", "Random Forest", "Trees", "Boosting")
tab <- xtable(table, digits = 2)
tab

lasso_plr
```

---
#### **Interactive Regression Model (IRM)**


```{r}
lgr::get_logger("mlr3")$set_threshold("warn")
dml_irm <- DoubleMLIRM$new(data_ml,
  ml_g = lasso,
  ml_m = lasso_class,
  trimming_threshold = 0.01, n_folds = 5
)
dml_irm$fit(store_predictions = TRUE)
dml_irm$summary()
lasso_irm <- dml_irm$coef
lasso_std_irm <- dml_irm$se


# predictions
dml_irm$params_names()
g0_hat <- as.matrix(dml_irm$predictions$ml_g0) # predictions of g_0(D=0, X)
g1_hat <- as.matrix(dml_irm$predictions$ml_g1) # predictions of g_0(D=1, X)
g_hat <- d * g1_hat + (1 - d) * g0_hat # predictions of g_0
m_hat <- as.matrix(dml_irm$predictions$ml_m) # predictions of m_o
```

```{r}
# cross-fitted RMSE: outcome
y <- as.matrix(pension$net_tfa) # true observations
d <- as.matrix(pension$e401)
lasso_y_irm <- sqrt(mean((y - g_hat)^2))
lasso_y_irm

# cross-fitted RMSE: treatment
lasso_d_irm <- sqrt(mean((d - m_hat)^2))
lasso_d_irm

# cross-fitted ce: treatment
mean(ifelse(m_hat > 0.5, 1, 0) != d)
```



```{r}
##### forest #####

dml_irm <- DoubleMLIRM$new(data_ml,
  ml_g = randomForest,
  ml_m = random_forest_class,
  trimming_threshold = 0.01, n_folds = 5
)
dml_irm$fit(store_predictions = TRUE)
dml_irm$summary()
forest_irm <- dml_irm$coef
forest_std_irm <- dml_plr$se

# predictions
g0_hat <- as.matrix(dml_irm$predictions$ml_g0) # predictions of g_0(D=0, X)
g1_hat <- as.matrix(dml_irm$predictions$ml_g1) # predictions of g_0(D=1, X)
g_hat <- d * g1_hat + (1 - d) * g0_hat # predictions of g_0
m_hat <- as.matrix(dml_irm$predictions$ml_m) # predictions of m_0

# cross-fitted RMSE: outcome
y <- as.matrix(pension$net_tfa) # true observations
d <- as.matrix(pension$e401)
forest_y_irm <- sqrt(mean((y - g_hat)^2))
forest_y_irm

# cross-fitted RMSE: treatment
forest_d_irm <- sqrt(mean((d - m_hat)^2))
forest_d_irm

# cross-fitted ce: treatment
mean(ifelse(m_hat > 0.5, 1, 0) != d)

##### trees #####

dml_irm <- DoubleMLIRM$new(data_ml,
  ml_g = trees, ml_m = trees_class,
  trimming_threshold = 0.01, n_folds = 5
)
dml_irm$fit(store_predictions = TRUE)
dml_irm$summary()
tree_irm <- dml_irm$coef
tree_std_irm <- dml_irm$se

# predictions
g0_hat <- as.matrix(dml_irm$predictions$ml_g0) # predictions of g_0(D=0, X)
g1_hat <- as.matrix(dml_irm$predictions$ml_g1) # predictions of g_0(D=1, X)
g_hat <- d * g1_hat + (1 - d) * g0_hat # predictions of g_0
m_hat <- as.matrix(dml_irm$predictions$ml_m) # predictions of m_o

# cross-fitted RMSE: outcome
y <- as.matrix(pension$net_tfa) # true observations
d <- as.matrix(pension$e401)
tree_y_irm <- sqrt(mean((y - g_hat)^2))
tree_y_irm

# cross-fitted RMSE: treatment
tree_d_irm <- sqrt(mean((d - m_hat)^2))
tree_d_irm

# cross-fitted ce: treatment
mean(ifelse(m_hat > 0.5, 1, 0) != d)


##### boosting #####

dml_irm <- DoubleMLIRM$new(data_ml,
  ml_g = boost, ml_m = boost_class,
  trimming_threshold = 0.01, n_folds = 5
)
dml_irm$fit(store_predictions = TRUE)
dml_irm$summary()
boost_irm <- dml_irm$coef
boost_std_irm <- dml_irm$se

# predictions
g0_hat <- as.matrix(dml_irm$predictions$ml_g0) # predictions of g_0(D=0, X)
g1_hat <- as.matrix(dml_irm$predictions$ml_g1) # predictions of g_0(D=1, X)
g_hat <- d * g1_hat + (1 - d) * g0_hat # predictions of g_0
m_hat <- as.matrix(dml_irm$predictions$ml_m) # predictions of m_o

# cross-fitted RMSE: outcome
y <- as.matrix(pension$net_tfa) # true observations
d <- as.matrix(pension$e401)
boost_y_irm <- sqrt(mean((y - g_hat)^2))
boost_y_irm

# cross-fitted RMSE: treatment
boost_d_irm <- sqrt(mean((d - m_hat)^2))
boost_d_irm

# cross-fitted ce: treatment
mean(ifelse(m_hat > 0.5, 1, 0) != d)
```



**まとめ**
```{r}
table <- matrix(0, 4, 4)
table[1, 1:4] <- c(lasso_irm, forest_irm, tree_irm, boost_irm)
table[2, 1:4] <- c(lasso_std_irm, forest_std_irm, tree_std_irm, boost_std_irm)
table[3, 1:4] <- c(lasso_y_irm, forest_y_irm, tree_y_irm, boost_y_irm)
table[4, 1:4] <- c(lasso_d_irm, forest_d_irm, tree_d_irm, boost_d_irm)
rownames(table) <- c("Estimate", "Std.Error", "RMSE Y", "RMSE D")
colnames(table) <- c("Lasso", "Random Forest", "Trees", "Boosting")
tab <- xtable(table, digits = 2)
tab
```


```{r}
lgr::get_logger("mlr3")$set_threshold("warn")
dml_irm <- DoubleMLIRM$new(data_ml,
  ml_g = randomForest,
  ml_m = lasso_class,
  trimming_threshold = 0.01, n_folds = 5
)
dml_irm$fit(store_predictions = TRUE)
dml_irm$summary()
best_irm <- dml_irm$coef
best_std_irm <- dml_irm$se
```

---

### **分析結果と考察**

#### **Table 10.4 Estimated Effect of 401(k) Eligibility on Net Financial Assets**

|                           | Lasso  | Tree   | Forest | Boost  | Best   | Ensemble |
|---------------------------|--------|--------|--------|--------|--------|----------|
| **A. Partially Linear Regression Model** |        |        |        |        |        |          |
| Estimate                  | 9418   | 8634   | 9112   | 8859   | 8859   | 9051     |
| Std. Error                | (1476) | (1303) | (1281) | (1321) | (1321) | (1315)   |
| RMSE D                    | 0.447  | 0.457  | 0.459  | 0.443  | 0.443  | 0.443    |
| RMSE Y                    | 58242  | 56819  | 55385  | 54153  | 54153  | 53917    |
| **B. Interactive Regression Model**     |        |        |        |        |        |          |
| Estimate                  | 8860   | 7856   | 8349   | 7871   | 8204   | 8146     |
| Std. Error                | (1347) | (1250) | (1502) | (1157) | (1144) | (1142)   |
| RMSE D                    | 0.448  | 0.457  | 0.459  | 0.443  | 0.443  | 0.443    |
| RMSE Y                    | 58300  | 54866  | 57293  | 55112  | 54866  | 53804    |

- 推定の補足
  - 傾向スコアの極端な値による推定への影響を防ぐため、(\(0.01\) と \(0.99\)) の範囲でトリミング

- **結果の概要**
   - 共変量を含まないモデルでは、401(k)への加入資格が金融資産に与える影響は19,559ドルと推定
   - 共変量をコントロールすることにより、理論的な予測通り、正の選択バイアスが補正される
   - PLM とIRM の結果は概ね一貫している
   - Poterba et al. によるシンプルなコントロールと結果は近く、それで十分である可能性を示唆している
- DAG で見たような、因果ダイアグラムのわずかな変更が識別に与える影響を分析するのは、Ch12 の sensitivity analysis で実施する